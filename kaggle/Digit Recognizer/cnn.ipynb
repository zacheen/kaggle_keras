{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 TensorFlow CNN tutorials 改成可以參加 kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嘗試紀錄\n",
    "# 1. 有 / 255 結果比較好\n",
    "# 2. 為了增加training data 所以減少val_split 結果比較好\n",
    "# 3. 加上 error_enhance 好一點點\n",
    "\n",
    "# Score: 0.99267\n",
    "# 很多前面排名的 有自己增加 dataset (加上 tensorflow 的 mnist data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:11:32.261134Z",
     "iopub.status.busy": "2022-01-26T05:11:32.260310Z",
     "iopub.status.idle": "2022-01-26T05:11:34.742110Z",
     "shell.execute_reply": "2022-01-26T05:11:34.741254Z"
    },
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "test_data_path = './input/test.csv'\n",
    "train_data_path = './input/train.csv'\n",
    "error_enhance_path = ['./input/error_enhance', '.csv']\n",
    "\n",
    "checkpoint_path = \"./checkpoint/train_checkpoint\"\n",
    "checkpoint_dir = \"./checkpoint\"\n",
    "final_checkpoint = './checkpoint/final_checkpoint'\n",
    "final_model = './saved_model/final_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training settings\n",
    "\n",
    "class CNN_set:\n",
    "    seed = 200 #67\n",
    "    using_enhance_data = True # 要在 training data 裡面加強 error data 嗎?\n",
    "    output_enhace = False # 要輸出 enhance error 資料嗎?\n",
    "    val_split = 0.001 # 要從 training data 裡面撥多少資料到 validation_data\n",
    "    train_from_begin = True  # False # True\n",
    "    save_checkPoint = False\n",
    "    lr = [(13, 0.001), (7, 0.0003), (4, 0.0001)] # learning_rate [(每次epoch次數, 使用的 learing rate)]\n",
    "    # 我看線圖 感覺原本lr就會慢慢下降 (因為每次換 lr 準確率會突然下降)\n",
    "        # 查資料看起來是固定的\n",
    "            # 應該是 momentum_optimizer_value 的問題 但目前不知道在哪裡設定\n",
    "        # 有一個參數可以設定 : decay 預設是0\n",
    "#     decay = 0.05\n",
    "    \n",
    "#     img_size = (28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and resize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料\n",
    "train_val_data = pd.read_csv(train_data_path)\n",
    "# 取得 train_data 的 label\n",
    "train_val_label = train_val_data['label']\n",
    "# 取得 train_data 的 image\n",
    "train_val_pic = train_val_data.iloc[:,1:].values.reshape(-1,28,28,1)  # 從1開始是因為第一行是 label \n",
    "\n",
    "print('train_val_label shape:', train_val_label.shape)\n",
    "print('train_val_pic shape:', train_val_pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add error enhance data (to balance the data, because I think odd written data isnt enough)\n",
    "have_enhance_data = False\n",
    "if CNN_set.using_enhance_data :\n",
    "    file_count = 1\n",
    "    while True :\n",
    "        each_error_path = error_enhance_path[0]+str(file_count)+error_enhance_path[1]\n",
    "    #     print(each_error_path)\n",
    "        if os.path.isfile(each_error_path) : \n",
    "            have_enhance_data = True\n",
    "            err_enh_data = pd.read_csv(each_error_path)\n",
    "            err_enh_label = err_enh_data['label']\n",
    "            # 取得 train_data 的 image\n",
    "            err_enh_pic = err_enh_data.iloc[:,1:].values.reshape(-1,28,28,1)  # 從1開始是因為第一行是 label \n",
    "            # 取得 test_data 的 image\n",
    "\n",
    "            print('err_enh_label shape:', err_enh_label.shape)\n",
    "            print('err_enh_pic shape:', err_enh_pic.shape)\n",
    "\n",
    "            for _ in range(3):  # 看要複製多少份\n",
    "                # err_enh 合併到 train_val\n",
    "                train_val_label = np.concatenate([train_val_label,err_enh_label],axis=0)\n",
    "                # 如果是 pd\n",
    "                # train_val_pic = train_val_pic.merge(err_enh_pic, how='outer', left_index=True, right_index=True)\n",
    "                train_val_pic = np.concatenate([train_val_pic,err_enh_pic],axis=0)\n",
    "        else :\n",
    "            break\n",
    "        file_count += 1\n",
    "\n",
    "print('train_val_label shape:', train_val_label.shape)\n",
    "print('train_val_pic shape:', train_val_pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 err_enh_pic 沒問題  只會印最後一個enhance檔案裏面的圖案\n",
    "if have_enhance_data :\n",
    "    class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    plt.figure(figsize=(7,2))\n",
    "    for i in range(min(14, len(err_enh_pic))):\n",
    "        plt.subplot(2,7,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(err_enh_pic[i], cmap=plt.get_cmap('gray'))\n",
    "        plt.xlabel(class_names[err_enh_label[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 train_val_pic 沒問題\n",
    "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "plt.figure(figsize=(7,2))  # 設定這個 plt 要顯示的大小\n",
    "for i in range(14):\n",
    "    plt.subplot(2,7,i+1)     # 切換到對應要顯示的位置\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_val_pic[i], cmap=plt.get_cmap('gray'))  # 這裡要注意不管有沒有 /255，印出來的圖片都會長的一樣\n",
    "    plt.xlabel(class_names[int(train_val_label[i])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 確定資料沒問題後 開始調整至符合 input 跟 output 的形狀\n",
    "train_val_pic = train_val_pic.reshape((train_val_label.shape[0], 28, 28, 1)).astype('float64') /255.0\n",
    "# train_val_pic = train_val_pic.reshape((42000, 28, 28, 1)).astype('float64') /255\n",
    "\n",
    "# 這裡不需要改變 lable 形狀\n",
    "# train_val_lable = to_categorical(train_val_lable, num_classes=class種類)\n",
    "\n",
    "print('train_val_pic shape:', train_val_pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通常會用 test_data 當作 valid_data\n",
    "# 但因為kaggle沒有給 所以就再從train_data分出來\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pic, val_pic , train_label, val_label = train_test_split(train_val_pic, train_val_label, \n",
    "    test_size=CNN_set.val_split, random_state=CNN_set.seed)\n",
    "    # random_state 如果想要重現一樣的結果 就代入同樣的數字 有點像 random seed\n",
    "\n",
    "# 這時候就不能再用 for 印 plot \n",
    "# 因為 dataset 已經分成兩組 且打亂了\n",
    "# 所以有些 位置 已經被抽走了 所以會造成 Key error\n",
    "    \n",
    "print('train_pic shape:', train_pic.shape)\n",
    "print('val_pic shape:', val_pic.shape)\n",
    "print('train_label shape:', train_label.shape)\n",
    "print('val_label shape:', val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作弊使用 tensorflow 的 mnist dataset\n",
    "# of course geting Score: 1.00000 # because model already knows the answer\n",
    "# conclusion : CNN model can handle this question, but cannot deal with unseen data \n",
    "\n",
    "# from tensorflow.python.keras.datasets import mnist\n",
    "# (train_pic, train_label), (val_pic, val_label) = mnist.load_data() # 讀進來是還沒 /255 的\n",
    "\n",
    "# train_pic = train_pic /255\n",
    "# val_pic = val_pic /255\n",
    "\n",
    "# train_pic = train_pic.reshape((train_pic.shape[0], 28, 28, 1))\n",
    "# val_pic = val_pic.reshape((val_pic.shape[0], 28, 28, 1))\n",
    "\n",
    "# # val_pic 通常不能和 train_pic 有交集\n",
    "# # 但這裡為了最大化 training data 所以加在一起\n",
    "# train_pic = np.concatenate([train_pic,val_pic],axis=0)\n",
    "# train_label = np.concatenate([train_label,val_label],axis=0)\n",
    "\n",
    "# print('train_pic shape:', train_pic.shape)\n",
    "# print('val_pic shape:', val_pic.shape)\n",
    "# print('train_label shape:', train_label.shape)\n",
    "# print(train_label)\n",
    "# print('val_label shape:', val_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKgyC5K_4O0d"
   },
   "source": [
    "# create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:11:49.708455Z",
     "iopub.status.busy": "2022-01-26T05:11:49.706738Z",
     "iopub.status.idle": "2022-01-26T05:11:51.276934Z",
     "shell.execute_reply": "2022-01-26T05:11:51.277363Z"
    },
    "id": "L9YmGQBQPrdn"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:11:51.324375Z",
     "iopub.status.busy": "2022-01-26T05:11:51.323638Z",
     "iopub.status.idle": "2022-01-26T05:11:51.347165Z",
     "shell.execute_reply": "2022-01-26T05:11:51.341496Z"
    },
    "id": "8Yu_m-TZUWGX"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if CNN_set.train_from_begin==False and latest != None :\n",
    "    print(\"using check points :\", latest)\n",
    "    model.load_weights(latest)\n",
    "else :\n",
    "    print(\"training from begin\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training process history\n",
    "his_acc = []\n",
    "his_val_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback_list = []\n",
    "\n",
    "# 建立 儲存 checkpoint 的 callback function\n",
    "# Create a callback that saves the model's weights \n",
    "save_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, save_weights_only=True, verbose=0)\n",
    "        # verbose 在 jupyter 好像沒什麼用\n",
    "        # verbose = 0 不輸出, 1 顯示進度條, 2 為每一個 epoch 輸出一條紀錄\n",
    "\n",
    "if CNN_set.save_checkPoint == False :\n",
    "    print(\"dont save checkpoint\")\n",
    "    cp_callback = None\n",
    "else :\n",
    "    cp_callback_list.append(save_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:11:51.354877Z",
     "iopub.status.busy": "2022-01-26T05:11:51.351479Z",
     "iopub.status.idle": "2022-01-26T05:12:53.567336Z",
     "shell.execute_reply": "2022-01-26T05:12:53.566859Z"
    },
    "id": "MdDzI75PUXrG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "for epoch, lr in CNN_set.lr:\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_pic, train_label, epochs=epoch, \n",
    "                        validation_data=(val_pic, val_label),\n",
    "                       callbacks=cp_callback_list)\n",
    "        # callbacks 如果有給的話 每幾分鐘會觸發一次\n",
    "        \n",
    "    his_acc += history.history['acc']\n",
    "    his_val_acc += history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認訓練完之後 再存一版 final\n",
    "if CNN_set.save_checkPoint :\n",
    "    model.save_weights(final_checkpoint)\n",
    "    model.save(final_model)\n",
    "    \n",
    "# model.save_weights(final_checkpoint)\n",
    "# model.save(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKgyC5K_4O0d"
   },
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(his_acc, label='accuracy')\n",
    "plt.plot(his_val_acc, label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.5, 1])  # 如果要顯示數據區間\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:12:53.592360Z",
     "iopub.status.busy": "2022-01-26T05:12:53.591680Z",
     "iopub.status.idle": "2022-01-26T05:12:54.645588Z",
     "shell.execute_reply": "2022-01-26T05:12:54.645130Z"
    },
    "id": "gtyDF0MKUcM7"
   },
   "outputs": [],
   "source": [
    "# check final accuracy\n",
    "test_loss, test_acc = model.evaluate(val_pic,  val_label, verbose=2)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出 val_pic 辨識錯誤的圖片並印出來\n",
    "diff_pic = []\n",
    "val_y_pred = model.predict(val_pic)\n",
    "val_y_pred = np.argmax(val_y_pred, axis=1)\n",
    "# print(y_pred)\n",
    "for pred_ans, real_ans, pic in zip(val_y_pred, val_label, val_pic):\n",
    "#     print(pred_ans, real_ans, pic)\n",
    "    if pred_ans != real_ans :\n",
    "        diff_pic.append((pred_ans, real_ans, pic))  # 因為讀進來後有先/255\n",
    "        \n",
    "# 印出\n",
    "rows = (len(diff_pic)//7)+1\n",
    "plt.figure(figsize=(7,rows+1))  # 設定這個 plt 要顯示的大小\n",
    "for i, (pred_ans, real_ans, pic) in enumerate(diff_pic):\n",
    "    plt.subplot(rows,7,i+1)     # 切換到對應要顯示的位置\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(pic, cmap=plt.get_cmap('gray'))\n",
    "    plt.xlabel(str(real_ans)+\"->\"+str(pred_ans))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提升這些錯誤資料的權重 (就是提升這些辨識錯誤的資料量)\n",
    "if CNN_set.output_enhace :\n",
    "    error_df = pd.DataFrame()\n",
    "    error_df['label'] = [x[1] for x in diff_pic]\n",
    "    error_pic = [x[2].reshape(784) for x in diff_pic]\n",
    "    error_pic = error_pic * 255\n",
    "    for i in range(784):\n",
    "        error_df['pixel'+str(i)] = [ error_pic[x][i] for x in range(len(diff_pic))]\n",
    "    error_df.to_csv('error_enhance.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得 test_pic 的 image\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_pic = test_data.iloc[:,:].values.reshape(-1,28,28,1)\n",
    "print('test_pic shape:', test_pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 test_pic 沒問題\n",
    "# test_pic 沒有 label\n",
    "# class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "plt.figure(figsize=(7,2))\n",
    "for i in range(14):\n",
    "    plt.subplot(2,7,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(test_pic[i], cmap=plt.get_cmap('gray'))\n",
    "#     plt.xlabel(class_names[test_label[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特別印出某個 test case\n",
    "# plt.imshow(test_pic[59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認沒問題後再塑形成可以丟入 input 的形狀\n",
    "test_pic = test_pic.reshape((test_pic.shape[0], 28, 28, 1)).astype('float64') /255.0 # 要注意!! 要不要 /255\n",
    "print('test_pic shape:', test_pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辨識結果\n",
    "y_pred = model.predict(test_pic)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把結果輸出到 csv\n",
    "submission_df = pd.DataFrame()\n",
    "image_id = [i for i in range(1, 28001)]\n",
    "submission_df['ImageId'] = image_id\n",
    "submission_df['Label'] = y_pred\n",
    "# print(submission_df)\n",
    "submission_df.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
